{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mysql-connector-python\n",
      "  Downloading mysql_connector_python-9.1.0-cp312-cp312-win_amd64.whl.metadata (6.2 kB)\n",
      "Downloading mysql_connector_python-9.1.0-cp312-cp312-win_amd64.whl (16.1 MB)\n",
      "   ---------------------------------------- 0.0/16.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/16.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/16.1 MB 3.3 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 1.0/16.1 MB 2.2 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.3/16.1 MB 2.0 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 1.6/16.1 MB 1.9 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.8/16.1 MB 1.7 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 2.1/16.1 MB 1.6 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 2.4/16.1 MB 1.5 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 2.9/16.1 MB 1.6 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 3.4/16.1 MB 1.6 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 3.7/16.1 MB 1.6 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 4.2/16.1 MB 1.7 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 4.5/16.1 MB 1.6 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 4.7/16.1 MB 1.6 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 5.0/16.1 MB 1.6 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 5.2/16.1 MB 1.6 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 5.8/16.1 MB 1.6 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 6.3/16.1 MB 1.6 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 6.6/16.1 MB 1.7 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 6.8/16.1 MB 1.7 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 7.1/16.1 MB 1.6 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 7.3/16.1 MB 1.6 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 7.9/16.1 MB 1.6 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 8.4/16.1 MB 1.6 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 8.4/16.1 MB 1.6 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 8.9/16.1 MB 1.6 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 9.2/16.1 MB 1.6 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 9.4/16.1 MB 1.6 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 9.7/16.1 MB 1.6 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 10.2/16.1 MB 1.6 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 10.2/16.1 MB 1.6 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 10.5/16.1 MB 1.6 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 10.7/16.1 MB 1.6 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 11.3/16.1 MB 1.6 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 11.5/16.1 MB 1.5 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 11.8/16.1 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 12.1/16.1 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 12.3/16.1 MB 1.5 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 13.1/16.1 MB 1.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 13.6/16.1 MB 1.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 13.9/16.1 MB 1.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 14.4/16.1 MB 1.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 14.4/16.1 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 14.9/16.1 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 15.2/16.1 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.5/16.1 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.0/16.1 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.1/16.1 MB 1.6 MB/s eta 0:00:00\n",
      "Installing collected packages: mysql-connector-python\n",
      "Successfully installed mysql-connector-python-9.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mysql-connector-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mysql.connector.cursor_cext.CMySQLCursor at 0x1cf53b6ea80>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mysql.connector\n",
    "connection = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password='pochi2002*',\n",
    "    database='Dataspark'\n",
    ")\n",
    "cursor = connection.cursor()  # Use 'cursor' here instead of 'curso'\n",
    "cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUSTOMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_customer_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS customer (\n",
    "    CustomerKey INT PRIMARY KEY,\n",
    "    Gender VARCHAR(10),\n",
    "    Name VARCHAR(255),\n",
    "    City VARCHAR(255),\n",
    "    StateCode VARCHAR(10),\n",
    "    State VARCHAR(255),\n",
    "    ZipCode VARCHAR(20),\n",
    "    Country VARCHAR(255),\n",
    "    Continent VARCHAR(50),\n",
    "    Birthday DATE\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_customer_table)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301, 'Female', 'Lilly Harding', 'WANDEARAH EAST', 'SA', 'South Australia', '5523', 'Australia', 'Australia', datetime.date(1939, 7, 3))\n",
      "(325, 'Female', 'Madison Hull', 'MOUNT BUDD', 'WA', 'Western Australia', '6522', 'Australia', 'Australia', datetime.date(1979, 9, 27))\n",
      "(554, 'Female', 'Claire Ferres', 'WINJALLOK', 'VIC', 'Victoria', '3380', 'Australia', 'Australia', datetime.date(1947, 5, 26))\n"
     ]
    }
   ],
   "source": [
    "customer_df = pd.read_csv(r'C:\\Users\\Priyanka Malavade\\Desktop\\Data Spark\\cleaned_dataset\\cleaned_customer.csv')\n",
    "customer_df = customer_df.where(pd.notnull(customer_df), None)\n",
    "\n",
    "for index, row in customer_df.iterrows():\n",
    "\n",
    "    insert_customer = \"\"\"\n",
    "    INSERT IGNORE INTO customer (CustomerKey, Gender, Name, City, StateCode, State, ZipCode, Country, Continent, Birthday)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    cursor.execute(insert_customer, (\n",
    "        row['CustomerKey'], row['Gender'], row['Name'], row['City'], row['State Code'], \n",
    "        row['State'], row['Zip Code'], row['Country'], row['Continent'], row['Birthday']\n",
    "    ))\n",
    "\n",
    "connection.commit()\n",
    "select_query = (\"SELECT * FROM customer LIMIT 3\") # Get the first 3 records for review\n",
    "cursor.execute(select_query)\n",
    "\n",
    "# Fetch and print the results\n",
    "result = cursor.fetchall()\n",
    "for row in result:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRODUCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "connection = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password='pochi2002*',\n",
    "    database='Dataspark'\n",
    ")\n",
    "cursor = connection.cursor()  # Use 'cursor' here instead of 'curso'\n",
    "cursor\n",
    "create_product_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS product (\n",
    "    ProductKey INT PRIMARY KEY,\n",
    "    ProductName VARCHAR(255),\n",
    "    Brand VARCHAR(100),\n",
    "    Color VARCHAR(50),\n",
    "    UnitCostUSD DECIMAL(10, 2),\n",
    "    UnitPriceUSD DECIMAL(10, 2),\n",
    "    SubcategoryKey INT,\n",
    "    Subcategory VARCHAR(255),\n",
    "    CategoryKey INT,\n",
    "    Category VARCHAR(255)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_product_table)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Contoso 512MB MP3 Player E51 Silver', 'Contoso', 'Silver', Decimal('6.62'), Decimal('12.99'), 101, 'MP4&MP3', 1, 'Audio')\n",
      "(2, 'Contoso 512MB MP3 Player E51 Blue', 'Contoso', 'Blue', Decimal('6.62'), Decimal('12.99'), 101, 'MP4&MP3', 1, 'Audio')\n",
      "(3, 'Contoso 1G MP3 Player E100 White', 'Contoso', 'White', Decimal('7.40'), Decimal('14.52'), 101, 'MP4&MP3', 1, 'Audio')\n"
     ]
    }
   ],
   "source": [
    "product_df = pd.read_csv(r'C:\\Users\\Priyanka Malavade\\Desktop\\Data Spark\\cleaned_dataset\\cleaned_products.csv')\n",
    "for index, row in product_df.iterrows():\n",
    "    insert_product = \"\"\"\n",
    "    INSERT IGNORE INTO product (ProductKey,ProductName,Brand,Color,UnitCostUSD,UnitPriceUSD,SubcategoryKey,Subcategory,CategoryKey,Category)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    cursor.execute(insert_product, (\n",
    "        row['ProductKey'], row['Product Name'], row['Brand'], row['Color'], row['Unit Cost USD'], \n",
    "        row['Unit Price USD'], row['SubcategoryKey'], row['Subcategory'], row['CategoryKey'], row['Category']\n",
    "    ))\n",
    "\n",
    "connection.commit()\n",
    "select_query = (\"SELECT * FROM product LIMIT 3\") # Get the first 3 records for review\n",
    "cursor.execute(select_query)\n",
    "\n",
    "# Fetch and print the results\n",
    "result = cursor.fetchall()\n",
    "for row in result:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXCHANGE RATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_exchange_rates_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS exchange_rates (\n",
    "    Date DATE PRIMARY KEY,\n",
    "    Currency VARCHAR(10),\n",
    "    Exchange DECIMAL(10, 4)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_exchange_rates_table)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(datetime.date(2015, 1, 1), 'USD', Decimal('1.0000'))\n",
      "(datetime.date(2015, 1, 2), 'USD', Decimal('1.0000'))\n",
      "(datetime.date(2015, 1, 3), 'GBP', Decimal('0.6477'))\n"
     ]
    }
   ],
   "source": [
    "exchange_df = pd.read_csv(r'C:\\Users\\Priyanka Malavade\\Desktop\\Data Spark\\cleaned_dataset\\cleaned_exchange_rates.csv')\n",
    "for index,row in exchange_df.iterrows():\n",
    "    insert_exchange_rates=\"\"\"\n",
    "    INSERT IGNORE INTO exchange_rates (Date,Currency,Exchange)\n",
    "    VALUES (%s, %s, %s)\n",
    "    \"\"\"\n",
    "    cursor.execute(insert_exchange_rates, (\n",
    "        row['Date'], row['Currency'], row['Exchange']\n",
    "    ))\n",
    "\n",
    "connection.commit()\n",
    "select_query = (\"SELECT * FROM exchange_rates LIMIT 3\") # Get the first 3 records for review\n",
    "cursor.execute(select_query)\n",
    "\n",
    "# Fetch and print the results\n",
    "result = cursor.fetchall()\n",
    "for row in result:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_sales_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS sales (\n",
    "    OrderNumber INT,\n",
    "    LineItem INT,\n",
    "    OrderDate DATE,\n",
    "    DeliveryDate DATE,\n",
    "    CustomerKey INT,\n",
    "    StoreKey INT,\n",
    "    ProductKey INT,\n",
    "    Quantity INT,\n",
    "    CurrencyCode VARCHAR(5),\n",
    "    PRIMARY KEY (OrderNumber, LineItem),\n",
    "    FOREIGN KEY (CustomerKey) REFERENCES customer(CustomerKey),\n",
    "    FOREIGN KEY (ProductKey) REFERENCES product(ProductKey)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_sales_table)\n",
    "connection.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(366000, 1, datetime.date(2016, 1, 1), None, 265598, 10, 1304, 1, 'CAD')\n",
      "(366001, 1, datetime.date(2016, 1, 1), datetime.date(2016, 1, 13), 1269051, 0, 1048, 2, 'USD')\n",
      "(366001, 2, datetime.date(2016, 1, 1), datetime.date(2016, 1, 13), 1269051, 0, 2007, 1, 'USD')\n"
     ]
    }
   ],
   "source": [
    "sale_df = pd.read_csv(r'C:\\Users\\Priyanka Malavade\\Desktop\\Data Spark\\cleaned_dataset\\cleaned_sales.csv')\n",
    "for index,row in sale_df.iterrows():\n",
    "    insert_sales=\"\"\"\n",
    "    INSERT IGNORE INTO sales (OrderNumber,LineItem,OrderDate,DeliveryDate,CustomerKey,StoreKey,ProductKey,Quantity,CurrencyCode)\n",
    "    VALUES (%s, %s, %s,%s, %s, %s,%s, %s, %s)\n",
    "    \"\"\"\n",
    "    cursor.execute(insert_sales, (\n",
    "        row['Order Number'], row['Line Item'], row['Order Date'],\n",
    "        row['Delivery Date'], row['CustomerKey'], row['StoreKey'],\n",
    "        row['ProductKey'], row['Quantity'], row['Currency Code']\n",
    "    ))\n",
    "\n",
    "connection.commit()\n",
    "select_query = (\"SELECT * FROM sales LIMIT 3\") # Get the first 3 records for review\n",
    "cursor.execute(select_query)\n",
    "\n",
    "# Fetch and print the results\n",
    "result = cursor.fetchall()\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MERGED SALES_PRODUCT_CUSTOMER TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_sale_product_customer_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS sale_product_customer (\n",
    "    OrderNumber INT,\n",
    "    LineItem INT,\n",
    "    OrderDate DATE,\n",
    "    DeliveryDate DATE,\n",
    "    CustomerKey INT,\n",
    "    StoreKey INT,\n",
    "    ProductKey INT,\n",
    "    Quantity INT,\n",
    "    CurrencyCode VARCHAR(10),\n",
    "    Gender VARCHAR(10),\n",
    "    Name VARCHAR(255),\n",
    "    City VARCHAR(255),\n",
    "    StateCode VARCHAR(10),\n",
    "    State VARCHAR(255),\n",
    "    ZipCode VARCHAR(20),\n",
    "    Country VARCHAR(255),\n",
    "    Continent VARCHAR(50),\n",
    "    Birthday DATE,\n",
    "    ProductName VARCHAR(255),\n",
    "    Brand VARCHAR(100),\n",
    "    Color VARCHAR(50),\n",
    "    UnitCostUSD DECIMAL(10, 2),\n",
    "    UnitPriceUSD DECIMAL(10, 2),\n",
    "    SubcategoryKey INT,\n",
    "    Subcategory VARCHAR(255),\n",
    "    CategoryKey INT,\n",
    "    Category VARCHAR(255),\n",
    "    PRIMARY KEY (OrderNumber, LineItem),\n",
    "    FOREIGN KEY (CustomerKey) REFERENCES customer(CustomerKey),\n",
    "    FOREIGN KEY (ProductKey) REFERENCES product(ProductKey),\n",
    "    FOREIGN KEY (StoreKey) REFERENCES stores(StoreKey)\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_sale_product_customer_table)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(366000, 1, datetime.date(2016, 1, 1), None, 265598, 10, 1304, 1, 'CAD', 'Male', 'Tyler Vaught', 'London', 'ON', 'Ontario', 'N5W 5K6', 'Canada', 'North America', datetime.date(1971, 3, 23), 'Contoso Lens Adapter M450 White', 'Contoso', 'White', Decimal('31.27'), Decimal('68.00'), 406, 'Cameras & Camcorders Accessories', 4, 'Cameras and camcorders')\n",
      "(366004, 1, datetime.date(2016, 1, 1), None, 1107461, 38, 163, 6, 'GBP', 'Male', 'Sam Nelson', 'BURSCOUGH BRIDGE', 'West Lanca', 'West Lancashire', 'L40 8UX', 'United Kingdom', 'Europe', datetime.date(1979, 11, 28), 'Adventure Works 52\" LCD HDTV X790W White', 'Adventure Works', 'White', Decimal('527.53'), Decimal('1592.20'), 201, 'Televisions', 2, 'TV and Video')\n",
      "(366004, 2, datetime.date(2016, 1, 1), None, 1107461, 38, 1529, 2, 'GBP', 'Male', 'Sam Nelson', 'BURSCOUGH BRIDGE', 'West Lanca', 'West Lancashire', 'L40 8UX', 'United Kingdom', 'Europe', datetime.date(1979, 11, 28), 'The Phone Company PDA Handheld 3.5 inch M610 Black', 'The Phone Company', 'Black', Decimal('117.27'), Decimal('255.00'), 504, 'Smart phones & PDAs', 5, 'Cell phones')\n"
     ]
    }
   ],
   "source": [
    "merged_sales_product_customer_df=pd.read_csv(r'C:\\Users\\Priyanka Malavade\\Desktop\\Data Spark\\cleaned_dataset\\merged_sales_customer_product_data.csv')\n",
    "merged_sales_product_customer_df = merged_sales_product_customer_df.where(pd.notnull(merged_sales_product_customer_df), None)\n",
    "\n",
    "\n",
    "for index,row in merged_sales_product_customer_df.iterrows():\n",
    "    insert_sale_product_customer = \"\"\"\n",
    "    INSERT IGNORE INTO sale_product_customer (\n",
    "        OrderNumber, LineItem, OrderDate, DeliveryDate, CustomerKey, StoreKey, ProductKey, Quantity, CurrencyCode, \n",
    "        Gender, Name, City, StateCode, State, ZipCode, Country, Continent, Birthday,\n",
    "        ProductName, Brand, Color, UnitCostUSD, UnitPriceUSD, SubcategoryKey, Subcategory, CategoryKey, Category\n",
    "    )\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, \n",
    "            %s, %s, %s, %s, %s, %s, %s, %s, %s, \n",
    "            %s, %s, %s, %s, %s, %s, %s, %s,%s)\n",
    "    \"\"\"\n",
    "    cursor.execute(insert_sale_product_customer, (\n",
    "        row['Order Number'], row['Line Item'], row['Order Date'], row['Delivery Date'], row['CustomerKey'], \n",
    "        row['StoreKey'], row['ProductKey'], row['Quantity'], row['Currency Code'], row['Gender'], row['Name'], \n",
    "        row['City'], row['State Code'], row['State'], row['Zip Code'], row['Country'], row['Continent'], row['Birthday'], \n",
    "        row['Product Name'], row['Brand'], row['Color'], row['Unit Cost USD'], row['Unit Price USD'], \n",
    "        row['SubcategoryKey'], row['Subcategory'], row['CategoryKey'], row['Category']\n",
    "    ))\n",
    "\n",
    "connection.commit()\n",
    "select_query = (\"SELECT * FROM sale_product_customer LIMIT 3\") # Get the first 3 records for review\n",
    "cursor.execute(select_query)\n",
    "\n",
    "# Fetch and print the results\n",
    "result = cursor.fetchall()\n",
    "for row in result:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor=connection.cursor()\n",
    "create_stores_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS stores (\n",
    "    StoreKey INT PRIMARY KEY,\n",
    "    Country VARCHAR(255),\n",
    "    State VARCHAR(255),\n",
    "    SquareMeters INT,\n",
    "    OpenDate DATE\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_stores_table)\n",
    "connection.commit()\n",
    "#StoreKey,Country,State,Square Meters,Open Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Priyanka Malavade\\AppData\\Local\\Temp\\ipykernel_13072\\614137866.py:15: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  store_df = store_df.applymap(lambda x: None if pd.isna(x) else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 1054 (42S22): Unknown column 'nan' in 'field list'\n",
      "(1, 'Australia', 'Australian Capital Territory', 595, datetime.date(2008, 1, 1))\n",
      "(2, 'Australia', 'Northern Territory', 665, datetime.date(2008, 1, 12))\n",
      "(3, 'Australia', 'South Australia', 2000, datetime.date(2012, 1, 7))\n",
      "(4, 'Australia', 'Tasmania', 2000, datetime.date(2010, 1, 1))\n",
      "(5, 'Australia', 'Victoria', 2000, datetime.date(2015, 12, 9))\n",
      "(6, 'Australia', 'Western Australia', 2000, datetime.date(2010, 1, 1))\n",
      "(7, 'Canada', 'New Brunswick', 1105, datetime.date(2007, 5, 7))\n",
      "(8, 'Canada', 'Newfoundland And Labrador', 2105, datetime.date(2014, 7, 2))\n",
      "(9, 'Canada', 'Northwest Territories', 1500, datetime.date(2005, 3, 4))\n",
      "(10, 'Canada', 'Nunavut', 1210, datetime.date(2015, 4, 4))\n",
      "(11, 'Canada', 'Yukon', 1210, datetime.date(2009, 6, 3))\n",
      "(12, 'France', 'Basse-Normandie', 350, datetime.date(2012, 6, 6))\n",
      "(13, 'France', 'Corse', 245, datetime.date(2013, 6, 7))\n",
      "(14, 'France', 'Franche-Comtã©', 350, datetime.date(2009, 12, 15))\n",
      "(15, 'France', 'La Rã©Union', 400, datetime.date(2015, 1, 1))\n",
      "(16, 'France', 'Limousin', 385, datetime.date(2010, 6, 3))\n",
      "(17, 'France', 'Martinique', 350, datetime.date(2007, 7, 8))\n",
      "(18, 'France', 'Mayotte', 310, datetime.date(2012, 8, 8))\n",
      "(19, 'Germany', 'Berlin', 1295, datetime.date(2015, 4, 4))\n",
      "(20, 'Germany', 'Brandenburg', 1715, datetime.date(2012, 12, 15))\n",
      "(21, 'Germany', 'Freie Hansestadt Bremen', 560, datetime.date(2018, 6, 3))\n",
      "(22, 'Germany', 'Freistaat Thã¼Ringen', 2000, datetime.date(2008, 3, 6))\n",
      "(23, 'Germany', 'Hamburg', 1365, datetime.date(2010, 1, 1))\n",
      "(24, 'Germany', 'Hessen', 1855, datetime.date(2012, 12, 15))\n",
      "(25, 'Germany', 'Mecklenburg-Vorpommern', 1610, datetime.date(2010, 1, 1))\n",
      "(26, 'Germany', 'Saarland', 350, datetime.date(2019, 3, 5))\n",
      "(27, 'Germany', 'Sachsen-Anhalt', 2000, datetime.date(2008, 8, 8))\n",
      "(28, 'Italy', 'Caltanissetta', 1200, datetime.date(2012, 12, 15))\n",
      "(29, 'Italy', 'Enna', 1000, datetime.date(2008, 1, 1))\n",
      "(30, 'Italy', 'Pesaro', 2100, datetime.date(2008, 1, 12))\n",
      "(31, 'Netherlands', 'Drenthe', 1085, datetime.date(2012, 1, 7))\n",
      "(32, 'Netherlands', 'Flevoland', 910, datetime.date(2010, 1, 1))\n",
      "(33, 'Netherlands', 'Friesland', 1540, datetime.date(2015, 12, 9))\n",
      "(34, 'Netherlands', 'Groningen', 1365, datetime.date(2010, 1, 1))\n",
      "(35, 'Netherlands', 'Zeeland', 1225, datetime.date(2007, 5, 7))\n",
      "(36, 'United Kingdom', 'Armagh', 1300, datetime.date(2014, 7, 2))\n",
      "(37, 'United Kingdom', 'Ayrshire', 2100, datetime.date(2005, 3, 4))\n",
      "(38, 'United Kingdom', 'Belfast', 1800, datetime.date(2015, 4, 4))\n",
      "(39, 'United Kingdom', 'Blaenau Gwent', 2100, datetime.date(2009, 6, 3))\n",
      "(40, 'United Kingdom', 'Dungannon And South Tyrone', 1300, datetime.date(2012, 6, 6))\n",
      "(41, 'United Kingdom', 'Fermanagh', 2100, datetime.date(2013, 6, 7))\n",
      "(42, 'United Kingdom', 'North Down', 1900, datetime.date(2009, 12, 15))\n",
      "(43, 'United States', 'Alaska', 1190, datetime.date(2015, 1, 1))\n",
      "(44, 'United States', 'Arkansas', 2000, datetime.date(2010, 6, 3))\n",
      "(45, 'United States', 'Connecticut', 2000, datetime.date(2007, 7, 8))\n",
      "(46, 'United States', 'Delaware', 1015, datetime.date(2012, 8, 8))\n",
      "(47, 'United States', 'Hawaii', 1120, datetime.date(2015, 4, 4))\n",
      "(48, 'United States', 'Idaho', 1540, datetime.date(2012, 12, 15))\n",
      "(49, 'United States', 'Iowa', 2000, datetime.date(2018, 6, 3))\n",
      "(50, 'United States', 'Kansas', 2000, datetime.date(2008, 3, 6))\n",
      "(51, 'United States', 'Maine', 1295, datetime.date(2010, 1, 1))\n",
      "(52, 'United States', 'Mississippi', 2000, datetime.date(2009, 6, 3))\n",
      "(53, 'United States', 'Montana', 1260, datetime.date(2012, 6, 6))\n",
      "(54, 'United States', 'Nebraska', 2000, datetime.date(2013, 6, 7))\n",
      "(55, 'United States', 'Nevada', 2000, datetime.date(2009, 12, 15))\n",
      "(56, 'United States', 'New Hampshire', 1260, datetime.date(2015, 1, 1))\n",
      "(57, 'United States', 'New Mexico', 1645, datetime.date(2010, 6, 3))\n",
      "(58, 'United States', 'North Dakota', 1330, datetime.date(2007, 7, 8))\n",
      "(59, 'United States', 'Oregon', 2000, datetime.date(2012, 8, 8))\n",
      "(60, 'United States', 'Rhode Island', 1260, datetime.date(2005, 4, 4))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "#  Read the CSV file\n",
    "store_df = pd.read_csv(r'C:\\Users\\Priyanka Malavade\\Desktop\\Data Spark\\cleaned_dataset\\cleaned_stores.csv')\n",
    "\n",
    "#  Drop rows with missing values in essential columns (StoreKey, Country, State)\n",
    "store_df = store_df.dropna(subset=['StoreKey', 'Country', 'State'], how='any')\n",
    "\n",
    "#  Drop rows where all values are NaN (empty rows)\n",
    "store_df = store_df.dropna(how='all')\n",
    "\n",
    "#  Replace NaN values with None for MySQL compatibility\n",
    "# The DataFrame.applymap ensures that NaN values are converted to None\n",
    "store_df = store_df.applymap(lambda x: None if pd.isna(x) else x)\n",
    "\n",
    "# Establish MySQL connection\n",
    "connection = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password='pochi2002*',\n",
    "    database='Dataspark'\n",
    ")\n",
    "cursor = connection.cursor() \n",
    "\n",
    "\n",
    "#  Insert cleaned data into MySQL\n",
    "for index, row in store_df.iterrows():\n",
    "    insert_stores = \"\"\"\n",
    "    INSERT IGNORE INTO stores (StoreKey, Country, State, SquareMeters, OpenDate)\n",
    "    VALUES (%s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    # Prepare the data as a tuple\n",
    "    data = (\n",
    "        row['StoreKey'], row['Country'], row['State'], row['Square Meters'], row['Open Date']\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        cursor.execute(insert_stores, data)\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error: {err}\")\n",
    "        # Optionally, log or handle specific errors\n",
    "\n",
    "# Commit the transaction to the database\n",
    "connection.commit()\n",
    "\n",
    "# Step 8: Optionally, verify the insertion by querying the first few rows\n",
    "select_query = \"SELECT * FROM stores LIMIT 60\"\n",
    "cursor.execute(select_query)\n",
    "result = cursor.fetchall()\n",
    "\n",
    "#  Print the results to verify the insertion\n",
    "for row in result:\n",
    "    print(row)\n",
    "\n",
    "#  Close the cursor and connection\n",
    "cursor.close()\n",
    "connection.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
